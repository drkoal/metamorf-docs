{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Metamorf","text":"<p>Metamorf is a Metadata Suite that generates SQL code within a framework based on select statements. Those queries are generated based on metadata that the user has entered on the framework. All this metadata is processed to maximize its potential, giving the user some features and facilities.</p> <p>It can execute and materialize these datasets, control the lineage end to end, even deploy an api sharing all the metadata, all in one. Thanks to its development, it is easy to deploy and use for anyone.</p> <p>It offers accelerators that allow you to develop data models in a faster and more controlled way, for example Datavault modelling. It offers, based on templates, the creation of entities in a standarized and scalable way, avoiding the possibility of human error.</p> <p>This entire frameworks was created and is maintained by Guillermo Aumatell Salom (guillermoaumatell@gmail.com). It is currently in beta phase constantly growing.</p> <pre><code>Current Version: 0.5.0\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":"<p>Metamorf is a Python Software that orchestrates, controls and transforms data. From a metadata entry, Metamorf process the metadata to full its system and in exchange grants a series of functionalities that allows to control all your datawarehouse.</p> <p>Metamorf stores all the metadata historically, it means that the users can see the status of their datawarehouse in the past at any time and know how the data was processed.</p> <p>The main benefit for use a framework to build all the data process, is to standarize all the processes, build modular and reusable data models and control all the data flow.</p> <p>Metadata and data can be on the same database or in separate ones, thus allowing it to be adaptaded to a greateer number of different architectures and needs.</p>"},{"location":"architecture/#databases","title":"Databases","text":"<p>Actually Metamorf supports the following databases for both data and metadata:</p> <ul> <li>Snowflake</li> <li>SQLite</li> <li>PostgreSQL</li> <li>MySQL</li> </ul>"},{"location":"architecture/#how-does-it-work","title":"How does it work?","text":"<p>Metamorf downloads as a python package, thus facilitating its installation in different environments. Its operation depends on a configuration file in YML format and a specific directory generated by the application itself. Once the environment has been prepared, any command can be executed through the shell.</p> <p>Once the developper has entered the metadata, the metadata needs to be processed. After a previous validation, the metadata is promoted on an exploitable metadata model. This final metadata permits to generate SQL models.</p> <p>Metamorf works with the concept of owner: it can be a user or a use case. All actions executed by metamorf are done selecting an owner, indicated on the configuration file. It can be useful for teamwork using the same Metadata Database and isolating each user or use case under development. Is up to you.</p> <p>It can fit with other technologies as dbt, giving the code ready to execute.</p>"},{"location":"commands/","title":"Commands","text":"<p>Metamorf can be executed:</p> <pre><code>    metamorf [command] [optional arguments]\n</code></pre>"},{"location":"commands/#help","title":"help","text":"<p>It shows all the commands available and some details.</p>"},{"location":"commands/#manifest","title":"manifest","text":"<p>It generates a Manifest Json File with all the information from the data models of the current owner.</p>"},{"location":"commands/#api","title":"api","text":"<p>It deploys an API that serve all the metadata information. Alpha Version, under development.</p>"},{"location":"commands/#validate","title":"validate","text":"<p>It validates the configuration file.</p>"},{"location":"commands/#init","title":"init","text":"<p>Metamorf Initialization on the current directory. Optional arguments [-d, --database, -m, --metadata]</p>"},{"location":"commands/#deploy","title":"deploy","text":"<p>It deploys on the metadata database all the requirements.</p>"},{"location":"commands/#deploy-example","title":"deploy-example","text":"<p>It deploys on the metadata database all the requirement with metadata information and example data on the data database an example.</p>"},{"location":"commands/#download","title":"download","text":"<p>It downloads the Metadata Entry of the Owner selected on CSV files, on the directory [entry]. Optional arguments [-s, --select, -o, --owner]</p>"},{"location":"commands/#upload","title":"upload","text":"<p>It uploads the Metadata Entry on the Metadata Database. Optional arguments [-s, --select]</p>"},{"location":"commands/#commit","title":"commit","text":"<p>It commits the actual metadata from your owner.</p>"},{"location":"commands/#recover","title":"recover","text":"<p>It recovers the last commit metadata of your owner. Optional arguments [-s, --select]</p>"},{"location":"commands/#process","title":"process","text":"<p>It processes all the metadata entry to promote the metadata.</p>"},{"location":"commands/#run","title":"run","text":"<p>It executes all the sql models. Optional arguments [-s, --select]</p>"},{"location":"commands/#output","title":"output","text":"<p>It generates the files of the processes on the output directory. Optional arguments [-s, --select]</p>"},{"location":"commands/#delete","title":"delete","text":"<p>It deletes all the metadata from the owner.</p>"},{"location":"commands/#backup","title":"backup","text":"<p>It downloads all the Metadata onto CSV Files to backup all the system. It allows you to recover them in the future using the restore command on an emergency or migration case.</p>"},{"location":"commands/#restore","title":"restore","text":"<p>It restores all the metadata from the backup folder of the repository.</p>"},{"location":"commands/#files","title":"files","text":"<p>It uploads all the files that are indicated on the metadata.</p>"},{"location":"commands/#metadata","title":"metadata","text":"<p>It validates the metadata entry.</p>"},{"location":"contact/","title":"Contact","text":"<p>Author: </p> <pre><code>Guillermo Aumatell Salom\n</code></pre> <p>Email: </p> <pre><code>guillermoaumatell@gmail.com\n</code></pre>"},{"location":"roadmap/","title":"Roadmap","text":"<p>This section presents the improvements that are in development and will be presented in the following versions.</p>"},{"location":"roadmap/#backlog","title":"Backlog","text":"<ul> <li>[Alpha Version] Documentation</li> <li>[Alpha Version] Visual Metadata Entry to draw Datavault Models instead of CSV entry system.</li> <li>[Near development] Include easier metadata entry system.</li> <li>[Near development] Add an engine system to permits developpers add their own engines to speed up their own needs.</li> <li>[Future development] Security Framework based on metadata for the datawarehouse</li> </ul>"},{"location":"setup/","title":"Set up","text":"<p>To install Metamorf, you need Python 3.10, and you can install it using pip. </p> <pre><code>pip install metamorf\n</code></pre> <p>Once installed, you can see all the available commands typing:</p> <pre><code>metamorf help\n</code></pre> <p>For a quick start, create a directory where you want to locate all the configuration files.</p> <pre><code>metamorf init\n</code></pre> <p>At this point, you have initialized your environment to configure Metamorf. The default configuration is through a SQLite database.</p> <p>Metamorf can be deployed where the configuration file says. If SQLite is selected (as default) and it doesn't exist, Metamorf will create the databases. Try it!</p> <pre><code>metamorf deploy\n</code></pre> <p>Now you have on your repository all the configuration files and a data and metadata example databases. To deploy an example on both databases, just type:</p> <pre><code>metamorf deploy-example\n</code></pre> <p>A fake dataset is deployed on the data database, and a metadata entry is inserted on the metadata database. You can just watch all these things with any Database Tool as DBeaver.  </p> <p>If you want to process these metadata, try the command:</p> <pre><code>metamorf process\n</code></pre> <p>Now all the metadata is processed and the main tables of Metamorf are populated. To see if all works, you can just type any of the features that Metamorf offers:</p> <p>If you want to run the processes indicated on the metadata to transform all the fake data, you can just type:</p> <pre><code>metadata run\n</code></pre> <p>Probably you just want the SQL Files to execute from other platforms, type then:</p> <pre><code>metadata output\n</code></pre> <p>The results are located on the output folder on your repository.  </p> <p>The configuration file permits you to configure Metamorf to adapt it to your needs. For example, one interesting option could be to generate all your datasets on dbt format. To achieve it, just change your configuration.yml file, output option, type value to dbt.</p> <pre><code>  output:\n    type: dbt\n</code></pre> <p>You can execute again </p> <pre><code>metadata output\n</code></pre> <p>And now you have all the necessary files for your dbt project.  </p> <p>This tutorial is just a small demonstration about metamorf and its potential.</p> <p>Note: Metamorf works on Windows OS and Linux (tested on Ubuntu)</p>"},{"location":"version_features/","title":"Version Features","text":""},{"location":"version_features/#050","title":"0.5.0","text":"<ul> <li>(Feature) New example based on datavault modelling.</li> <li>(Feature) Validations for metadata.</li> <li>(Feature) Creation of schemas and databases if needed.</li> <li>(Feature) Add new columns if needed.</li> <li>(Feature) Select process to execute with dependencies if needed.</li> <li>(Feature) Configuration hashes for Datavault module.</li> <li>(Feature) Execution with threads.</li> <li>(Feature) Primary Key on create table.</li> <li>(Feature) Delete, Update, Merge strategies.</li> <li>(Bugfix) Fixed some small bugs.</li> </ul>"},{"location":"version_features/#0442","title":"0.4.4.2","text":"<ul> <li>(Bugfix) Execute any commands without any config file.</li> <li>(Bugfix) Errors on NULL or None values on metadata entry.</li> <li>(Bugfix) Errors on column detection on complex mappings.</li> <li>(Bugfix) 'Files' command not using the correct target path connection.</li> <li>(Bugfix) Error on historical metadata when delete some entities in special casuistics.</li> <li>(Feature) Added 'if is null' logic on all hashes.</li> <li>(Feature) The user can change column names on Datavault Engine - Mappings.</li> <li>(Feature) Added duplicate control on Datavault Engine - Satellites.</li> </ul>"},{"location":"version_features/#0441","title":"0.4.4.1","text":"<ul> <li>(Feature) Datavault module to speed up the creation of a Datavault Model. New metadata entry was added.</li> <li>(Feature) File loader to upload csv files to the data database.</li> <li>(Feature) Added compatibility to PostgreSQL and MySQL for metadata and data databases.</li> <li>(Feature) Added compatibility to Linux OS.</li> </ul>"},{"location":"version_features/#0431","title":"0.4.3.1","text":"<ul> <li>(Bugfix) on installation with some packages.</li> </ul>"}]}